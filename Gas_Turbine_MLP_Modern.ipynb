{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30dfdb9b",
   "metadata": {},
   "source": [
    "\n",
    "# Gas Turbine Emissions Prediction (CO & NOx)\n",
    "\n",
    "**Modernized implementation using:**\n",
    "- Scikit-learn Pipelines\n",
    "- GridSearchCV\n",
    "- StandardScaler\n",
    "- MLPRegressor\n",
    "- Clean train/validation/test split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ef210f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import optuna, shap, warnings, tensorflow as tf\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2ba74f",
   "metadata": {},
   "source": [
    "# 1. DATA INGESTION & TEMPORAL SPLIT (Preventing Autocorrelation Leakage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83b4069c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m     feat \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAT\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAP\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAH\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAFDP\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGTEP\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTIT\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTAT\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTEY\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCDP\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m train_df[feat], train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNOX\u001b[39m\u001b[38;5;124m'\u001b[39m], val_df[feat], val_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNOX\u001b[39m\u001b[38;5;124m'\u001b[39m], test_df[feat], test_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNOX\u001b[39m\u001b[38;5;124m'\u001b[39m], feat\n\u001b[0;32m---> 13\u001b[0m X_train, y_train, X_val, y_val, X_test, y_test, feat_names \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[1;32m     16\u001b[0m X_train_s \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(X_train)\n",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m, in \u001b[0;36mload_and_split\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_and_split\u001b[39m():\n\u001b[1;32m      2\u001b[0m     files \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgt_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2011\u001b[39m, \u001b[38;5;241m2016\u001b[39m)]\n\u001b[0;32m----> 3\u001b[0m     data \u001b[38;5;241m=\u001b[39m [pd\u001b[38;5;241m.\u001b[39mread_csv(f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m files]\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Per Heybak et al.: Train (11-12), Val (13), Test (14-15)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     train_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(data[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m2\u001b[39m])\n",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_and_split\u001b[39m():\n\u001b[1;32m      2\u001b[0m     files \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgt_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2011\u001b[39m, \u001b[38;5;241m2016\u001b[39m)]\n\u001b[0;32m----> 3\u001b[0m     data \u001b[38;5;241m=\u001b[39m [\u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m files]\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Per Heybak et al.: Train (11-12), Val (13), Test (14-15)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     train_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(data[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m2\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_and_split():\n",
    "    files = [f\"gt_{y}.csv\" for y in range(2011, 2016)]\n",
    "    data = [pd.read_csv(f) for f in files]\n",
    "    \n",
    "    # Per Heybak et al.: Train (11-12), Val (13), Test (14-15)\n",
    "    train_df = pd.concat(data[0:2])\n",
    "    val_df = data[2]\n",
    "    test_df = pd.concat(data[3:5])\n",
    "    \n",
    "    feat = ['AT', 'AP', 'AH', 'AFDP', 'GTEP', 'TIT', 'TAT', 'TEY', 'CDP']\n",
    "    return train_df[feat], train_df['NOX'], val_df[feat], val_df['NOX'], test_df[feat], test_df['NOX'], feat\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, feat_names = load_and_split()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_val_s = scaler.transform(X_val)\n",
    "X_test_s = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2dd9f2",
   "metadata": {},
   "source": [
    "# 2. BAYESIAN OPTIMIZATION (Optuna)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26744f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "        'hidden_layer_sizes': (trial.suggest_int('u1', 64, 128), trial.suggest_int('u2', 32, 64)),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-4, 1e-1, log=True),\n",
    "        'learning_rate_init': trial.suggest_float('lr', 1e-4, 1e-2, log=True)\n",
    "    }\n",
    "    model = MLPRegressor(**params, max_iter=200, random_state=42).fit(X_train_s, y_train)\n",
    "    return model.score(X_val_s, y_val)\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e004c4",
   "metadata": {},
   "source": [
    "# 3. HYBRID STACKING ENSEMBLE (MLP + XGBoost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f186b31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mlp = MLPRegressor(hidden_layer_sizes=(study.best_params['u1'], study.best_params['u2']), \n",
    "                        alpha=study.best_params['alpha'], random_state=42)\n",
    "\n",
    "stacking_pems = StackingRegressor(\n",
    "    estimators=[('mlp', best_mlp), ('xgb', XGBRegressor(n_estimators=200, max_depth=5))],\n",
    "    final_estimator=Ridge(alpha=1.0)\n",
    ").fit(X_train_s, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b002548c",
   "metadata": {},
   "source": [
    "# 4. ISOTONIC CALIBRATION & UNCERTAINTY (Platt Scaling Equivalent for Regression)\n",
    "# Predict Intervals (Quantile Regression)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2858a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_model = XGBRegressor(objective='reg:quantileerror', quantile_alpha=0.95).fit(X_train_s, y_train)\n",
    "lower_model = XGBRegressor(objective='reg:quantileerror', quantile_alpha=0.05).fit(X_train_s, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0ca0e2",
   "metadata": {},
   "source": [
    "# Calibrate Mean Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84ad22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "iso_calib = IsotonicRegression(out_of_bounds='clip').fit(stacking_pems.predict(X_val_s), y_val)\n",
    "final_preds = iso_calib.predict(stacking_pems.predict(X_test_s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af5fd42",
   "metadata": {},
   "source": [
    "# 5. DEEP LEARNING (LSTM for Sequential Memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7f18b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_seq(X, y, steps=5):\n",
    "    X_s, y_s = [], []\n",
    "    for i in range(len(X)-steps):\n",
    "        X_s.append(X[i:(i+steps)])\n",
    "        y_s.append(y.iloc[i+steps])\n",
    "    return np.array(X_s), np.array(y_s)\n",
    "\n",
    "X_train_seq, y_train_seq = create_seq(X_train_s, y_train)\n",
    "X_test_seq, y_test_seq = create_seq(X_test_s, y_test)\n",
    "\n",
    "lstm_pems = Sequential([\n",
    "    Input(shape=(5, 9)),\n",
    "    LSTM(50, return_sequences=True),\n",
    "    LSTM(25),\n",
    "    Dense(1)\n",
    "])\n",
    "lstm_pems.compile(optimizer='adam', loss='mae')\n",
    "lstm_pems.fit(X_train_seq, y_train_seq, epochs=10, batch_size=64, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b68f57",
   "metadata": {},
   "source": [
    "# 6. SOTA VISUALIZATION & XAI (SHAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307faff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"PEMS Stacking R2: {r2_score(y_test, final_preds):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a25664b",
   "metadata": {},
   "source": [
    "# Uncertainty Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467d6c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.fill_between(range(100), lower_model.predict(X_test_s[:100]), upper_model.predict(X_test_s[:100]), alpha=0.2, label='90% Confidence (PEMS Uncertainty)')\n",
    "plt.plot(final_preds[:100], label='Predicted NOx (Calibrated)')\n",
    "plt.scatter(range(100), y_test[:100], color='red', s=10, label='Actual Sensor')\n",
    "plt.title(\"SOTA PEMS: Prediction with Uncertainty Intervals\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8abfe1",
   "metadata": {},
   "source": [
    "# SHAP Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893d3b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(stacking_pems.predict, X_train_s[:100])\n",
    "shap_values = explainer(X_test_s[:300])\n",
    "shap.summary_plot(shap_values, X_test_s[:300], feature_names=feat_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
